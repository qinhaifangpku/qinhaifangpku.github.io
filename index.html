<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Haifang Qin - Personal Homepage (Liquid Design)</title>
    <style>
        @keyframes gradientFlow {
            0% { background-position: 0% 50%; }
            50% { background-position: 100% 50%; }
            100% { background-position: 0% 50%; }
        }

        body {
            font-family: 'Segoe UI', Tahoma, Geneva, Verdana, sans-serif;
            line-height: 1.7;
            margin: 0;
            padding: 0;
            color: #e0e0e0; /* Light text color for contrast with dark gradient */
            background: linear-gradient(-45deg, #0d0c2c, #242458, #3a3a8a, #794686, #b25ca4, #dc7db4);
            /* Darker, sophisticated liquid gradient */
            background-size: 600% 600%;
            animation: gradientFlow 25s ease infinite;
            display: flex;
            flex-direction: column;
            align-items: center;
            min-height: 100vh;
            padding: 20px 0; /* Add some padding top and bottom for scroll */
        }

        .ppt-container {
            width: 90%;
            max-width: 1400px; /* Widescreen friendly */
            margin: 20px auto;
        }

        .section {
            background: rgba(255, 255, 255, 0.07); /* Subtle frosted glass */
            margin-bottom: 30px;
            padding: 25px 35px;
            border-radius: 35px 15px 45px 15px; /* Organic shape */
            box-shadow: 0 10px 30px rgba(0, 0, 0, 0.25);
            backdrop-filter: blur(10px);
            border: 1px solid rgba(255, 255, 255, 0.1);
            transition: transform 0.3s ease-out, box-shadow 0.3s ease-out;
        }

        .section:hover {
            transform: translateY(-5px) scale(1.005);
            box-shadow: 0 15px 40px rgba(0, 0, 0, 0.3);
        }

        h1, h2, h3 {
            color: #ffffff;
            text-shadow: 0 0 8px rgba(0,0,0,0.2);
        }

        h1 {
            font-size: 2.6em;
            margin-bottom: 5px;
        }

        h2 { /* Section titles */
            font-size: 2em;
            margin-top: 0;
            margin-bottom: 25px;
            padding-bottom: 10px;
            border-bottom: 2px solid rgba(255, 255, 255, 0.2);
            display: inline-block;
        }
        
        h3 { /* Subsection titles like publication title */
            font-size: 1.3em;
            color: #f0f8ff; /* AliceBlue for a soft touch */
            margin-top: 0;
            margin-bottom: 8px;
            font-style: italic;
        }

        a {
            color: #89f7fe; /* Bright cyan for links */
            text-decoration: none;
            transition: color 0.3s ease, text-shadow 0.3s ease;
        }

        a:hover {
            color: #a7faff;
            text-shadow: 0 0 5px rgba(137, 247, 254, 0.5);
        }

        /* Profile Section */
        .profile-header {
            display: flex;
            align-items: center;
            gap: 30px;
            flex-wrap: wrap; /* Allow wrapping on smaller screens */
        }

        .profile-image-container {
            flex-shrink: 0;
        }

        .profile-image {
            width: 180px; /* Adjust as needed */
            height: 180px;
            border-radius: 40% 60% 55% 45% / 50% 45% 55% 50%; /* Organic blob shape */
            object-fit: cover;
            box-shadow: 0 5px 20px rgba(0,0,0,0.3);
            border: 3px solid rgba(255,255,255,0.2);
            transition: transform 0.4s ease, border-radius 0.4s ease;
        }
        .profile-image:hover {
            transform: scale(1.05);
            border-radius: 50%; /* Smoothen on hover */
        }

        .profile-info p {
            margin: 8px 0;
            font-size: 1.1em;
        }
        .profile-info .name-chinese {
            font-size: 1.3em;
            color: #e5e5e5;
        }
        .profile-info .email-img {
            vertical-align: middle;
            height: 1.2em; /* Adjust based on your image's aspect ratio */
        }

        /* About Section */
        .about-me p {
            font-size: 1.1em;
            line-height: 1.8;
        }

        /* Publications Section */
        .publications-list {
            list-style: none;
            padding: 0;
        }
        .publication-item {
            display: flex;
            gap: 20px;
            margin-bottom: 25px;
            padding: 20px;
            background: rgba(0,0,0,0.1);
            border-radius: 20px 10px 25px 10px;
            transition: background-color 0.3s ease;
        }
        .publication-item:hover {
            background: rgba(0,0,0,0.18);
        }

        .pub-accent-bar {
            width: 8px;
            flex-shrink: 0;
            background: linear-gradient(to bottom, #dc7db4, #794686, #3a3a8a); /* Gradient accent */
            border-radius: 10px;
        }
        .pub-details {
            flex-grow: 1;
        }
        .pub-authors {
            font-size: 0.95em;
            color: #c0c0c0; /* Lighter gray for authors */
            margin-bottom: 5px;
        }
        .pub-venue {
            font-size: 0.9em;
            color: #b0b0b0;
            margin-bottom: 10px;
        }
        .pub-links a {
            display: inline-block;
            margin-right: 15px;
            padding: 6px 12px;
            background-color: rgba(137, 247, 254, 0.1);
            border: 1px solid rgba(137, 247, 254, 0.3);
            color: #89f7fe;
            border-radius: 50px; /* Pill shape */
            font-size: 0.85em;
            font-weight: 500;
            transition: all 0.3s ease;
        }
        .pub-links a:hover {
            background-color: rgba(137, 247, 254, 0.25);
            border-color: rgba(137, 247, 254, 0.6);
            transform: translateY(-2px);
            text-shadow: none;
        }

        /* Awards Section */
        .awards-list {
            list-style: none;
            padding-left: 0;
        }
        .awards-list li {
            padding: 10px 0 10px 30px; /* Indent for icon */
            font-size: 1.05em;
            position: relative;
            border-bottom: 1px solid rgba(255,255,255,0.1);
        }
        .awards-list li:last-child {
            border-bottom: none;
        }
        .awards-list li::before {
            content: 'üèÜ'; /* Trophy emoji as bullet */
            position: absolute;
            left: 0;
            top: 50%;
            transform: translateY(-50%);
            font-size: 1.2em;
            color: #ffd700; /* Gold color for trophy */
        }
        .awards-list li a {
            font-weight: 500;
        }

        .google-scholar-link {
            display: inline-block;
            padding: 8px 15px;
            background-color: rgba(255,255,255,0.08);
            border-radius: 20px;
            margin-bottom: 20px;
            font-weight: bold;
            transition: background-color 0.3s ease;
        }
        .google-scholar-link:hover {
             background-color: rgba(255,255,255,0.15);
        }

        /* Footer */
        footer {
            text-align: center;
            margin-top: 30px;
            padding: 20px;
            font-size: 0.9em;
            color: rgba(255,255,255,0.6);
        }

    </style>
</head>
<body>
    <div class="ppt-container">
        <header class="section profile-header">
            <div class="profile-image-container">
                <img src="./new_me.png" alt="Haifang Qin" class="profile-image">
            </div>
            <div class="profile-info">
                <h1>Haifang Qin <span class="name-chinese">Áß¶Êµ∑Ëä≥</span></h1>
                <p>Computer Vision and Machine Learning Researcher</p>
                <p>Disney+Hotstar Ltd, Beijing, China</p>
                <p><img src="./new_email.png" alt="Email" class="email-img"></p>
            </div>
        </header>

        <section class="section about-me">
            <h2>About Me</h2>
            <p>
                Hello, I'm Haifang. I am currently working at Disney+, focusing mainly on Computer Vision area. I completed my Bachelor's degree at Peking University in 2016, and later pursued my Master's degree under the guidance of Prof. Yuru Pei, which I obtained in 2019. Following that, I worked at Sensetime Ltd for around 2 years, where I specialized in autonomous driving with a primary focus on perception research.
            </p>
            <p>
                My research interests are diverse and encompass two areas: object detection and instance segmentation of common objects, as well as medical image analysis involving registration and segmentation. I am deeply committed to making a meaningful impact in these fields and actively seek opportunities for collaboration.
            </p>
        </section>

        <section class="section publications">
            <h2>Publications</h2>
            <p><a href="https://scholar.google.com/citations?user=53CawLYAAAAJ&hl=en" target="_blank" class="google-scholar-link">View Google Scholar Profile ‚ÜóÔ∏è</a></p>
            <ul class="publications-list">
                <li class="publication-item">
                    <div class="pub-accent-bar"></div>
                    <div class="pub-details">
                        <h3>Deformable registration of lateral cephalogram and cone‚Äëbeam computed tomography image</h3>
                        <p class="pub-authors">Yungeng Zhang, <strong>Haifang Qin</strong>, Peixin Li, Yuru Pei, Yuke Guo, Tianmin Xu, and Hongbin Zha</p>
                        <p class="pub-venue">Medical Physics, 2021</p>
                        <p class="pub-links"><a href="https://pubmed.ncbi.nlm.nih.gov/34496039/" target="_blank">Paper</a></p>
                    </div>
                </li>
                <li class="publication-item">
                    <div class="pub-accent-bar"></div>
                    <div class="pub-details">
                        <h3>Image hashing via linear discriminant learning</h3>
                        <p class="pub-authors">Weixiang Hong, Yu-Ting Chang, <strong>Haifang Qin</strong>, Wei-Chih Hung, Yi-Hsuan Tsai, and Ming-Hsuan Yang</p>
                        <p class="pub-venue">Proceedings of the IEEE/CVF winter conference on applications of computer vision (<strong>WACV</strong>), 2020</p>
                        <p class="pub-links"><a href="https://openaccess.thecvf.com/content_WACV_2020/papers/Hong_Image_Hashing_via_Linear_Discriminant_Learning_WACV_2020_paper.pdf" target="_blank">Paper</a></p>
                    </div>
                </li>
                <li class="publication-item">
                    <div class="pub-accent-bar"></div>
                    <div class="pub-details">
                        <h3>Masseter Muscle Segmentation from Cone-Beam CT Images using Generative Adversarial Network</h3>
                        <p class="pub-authors">Yungeng Zhang, Yuru Pei, <strong>Haifang Qin</strong>, Yuke Guo, Gengyu Ma,Tianmin Xu and Hongbin Zha</p>
                        <p class="pub-venue">International Symposium on Biomedical Imaging (<strong>ISBI</strong>), 2019</p>
                        <p class="pub-links"><a href="https://ieeexplore.ieee.org/document/8759426" target="_blank">Paper</a></p>
                    </div>
                </li>
                 <li class="publication-item">
                    <div class="pub-accent-bar"></div>
                    <div class="pub-details">
                        <h3>A Top-Down Unified Framework for Instance-level Human Parsing</h3>
                        <p class="pub-authors"><strong>Haifang Qin</strong>*, Weixiang Hong*, Wei-Chih Hung, Yi-Hsuan Tsai, and Ming-Hsuan Yang</p>
                        <p class="pub-venue">British Machine Vision Conference (<strong>BMVC</strong>), 2019</p>
                        <p class="pub-links"><a href="https://faculty.ucmerced.edu/mhyang/papers/bmvc2019_human_parsing.pdf" target="_blank">Paper</a></p>
                    </div>
                </li>
                <li class="publication-item">
                    <div class="pub-accent-bar"></div>
                    <div class="pub-details">
                        <h3>Masseter segmentation from computed tomography using feature-enhanced nested residual neural network</h3>
                        <p class="pub-authors"><strong>Haifang Qin</strong>, Yuru Pei, Yuke Guo, Gengyu Ma, Tianmin Xu, and Hongbin Zha</p>
                        <p class="pub-venue">Machine Learning in Medical Imaging (<strong>MLMI</strong>), 2018</p>
                        <p class="pub-links"><a href="https://link.springer.com/chapter/10.1007/978-3-030-00919-9_41" target="_blank">Paper</a></p>
                    </div>
                </li>
                <li class="publication-item">
                    <div class="pub-accent-bar"></div>
                    <div class="pub-details">
                        <h3>Path Aggregation Network for Instance Segmentation</h3>
                        <p class="pub-authors">Shu Liu*, Lu Qi*, <strong>Haifang Qin</strong>*, Jianping Shi<sup>‚Ä†</sup>, and Jiaya Jia<sup>‚Ä†</sup></p>
                        <p class="pub-venue">Computer Vision and Pattern Recognition (<strong>CVPR</strong>), 2018 (<strong>Spotlight</strong>)</p>
                        <p class="pub-links">
                            <a href="https://arxiv.org/abs/1803.01534" target="_blank">Paper</a>
                            <a href="https://github.com/ShuLiu1993/PANet" target="_blank">Code</a>
                        </p>
                    </div>
                </li>
                <li class="publication-item">
                    <div class="pub-accent-bar"></div>
                    <div class="pub-details">
                        <h3>Temporal consistent 2D-3D registration of lateral cephalograms and cone-beam computed tomography images</h3>
                        <p class="pub-authors">Yungeng Zhang, Yuru Pei, <strong>Haifang Qin</strong>, Yuke Guo, Gengyu Ma, Tianmin Xu, and Hongbin Zha</p>
                        <p class="pub-venue">Machine Learning in Medical Imaging (<strong>MLMI</strong>), 2018</p>
                        <p class="pub-links"><a href="https://link.springer.com/chapter/10.1007/978-3-030-00919-9_43" target="_blank">Paper</a></p>
                    </div>
                </li>
                 <li class="publication-item">
                    <div class="pub-accent-bar"></div>
                    <div class="pub-details">
                        <h3>Multi-scale volumetric ConvNet with nested residual connections for segmentation of anterior cranial base</h3>
                        <p class="pub-authors">Yuru Pei, <strong>Haifang Qin</strong>, Gengyu Ma, Yuke Guo, Gui Chen, Tianmin Xu, and Hongbin Zha</p>
                        <p class="pub-venue">Machine Learning in Medical Imaging (<strong>MLMI</strong>), 2017</p>
                        <p class="pub-links"><a href="https://link.springer.com/chapter/10.1007/978-3-319-67389-9_15" target="_blank">Paper</a></p>
                    </div>
                </li>
                <li class="publication-item">
                    <div class="pub-accent-bar"></div>
                    <div class="pub-details">
                        <h3>Non-rigid craniofacial 2D-3D registration using CNN-based regression</h3>
                        <p class="pub-authors">Yuru Pei, Yungeng Zhang, <strong>Haifang Qin</strong>*, Gengyu Ma Yuke Guo, Tianmin Xu, and Hongbin Zha*</p>
                        <p class="pub-venue">Deep Learning in Medical Image Analysis and Multimodal Learning for Clinical Decision Support: Third International Workshop (<strong>DLMIA</strong>), 2017</p>
                        <p class="pub-links"><a href="https://link.springer.com/chapter/10.1007/978-3-319-67558-9_14" target="_blank">Paper</a></p>
                    </div>
                </li>
            </ul>
        </section>

        <section class="section awards">
            <h2>Awards and Honors</h2>
            <ul class="awards-list">
                <li><a href="https://places-coco2017.github.io/#winners" target="_blank">No. 1 of COCO 2017 Detection Challenge in Instance Segmentation Track</a>, 2017</li>
                <li><a href="https://places-coco2017.github.io/#winners" target="_blank">No. 2 of COCO 2017 Detection Challenge in Object Detection Track</a>, 2017</li>
                <li><a href="https://research.mapillary.com/lsun.html#fn:2" target="_blank">No. 1 of LSUN'17 Instance Segmentation Challenge</a>, 2017</li>
                <li>No. 3 of SpaceNet Challenge in Topcoder 2017, 2017</li>
            </ul>
        </section>
        
        <footer>
            <p>&copy; Haifang Qin | Last updated: May 2024 (Content from original page) <br> Liquid Interface Design for Presentation</p>
        </footer>
    </div>
</body>
</html>
